{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import json\n",
    "import tensorflowjs as tfjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./trainingData/sherlockS.txt\", 'r', encoding='utf-8') as myfile:\n",
    "    mytext = myfile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytokenizer = Tokenizer()\n",
    "mytokenizer.fit_on_texts([mytext])\n",
    "total_words = len(mytokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input_sequences = []\n",
    "for line in mytext.split('\\n'):\n",
    "    token_list = mytokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        my_n_gram_sequence = token_list[:i+1]\n",
    "        my_input_sequences.append(my_n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in my_input_sequences])\n",
    "input_sequences = np.array(pad_sequences(my_input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'input_length')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lusti\\OneDrive\\Dokumente\\Studium\\Deep Learning\\RNN\\trainingFFNN.ipynb Zelle 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lusti/OneDrive/Dokumente/Studium/Deep%20Learning/RNN/trainingFFNN.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m Sequential()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lusti/OneDrive/Dokumente/Studium/Deep%20Learning/RNN/trainingFFNN.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39madd(Dense(total_words, \u001b[39m100\u001b[39;49m, input_length\u001b[39m=\u001b[39;49mmax_sequence_len\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lusti/OneDrive/Dokumente/Studium/Deep%20Learning/RNN/trainingFFNN.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39m150\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lusti/OneDrive/Dokumente/Studium/Deep%20Learning/RNN/trainingFFNN.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39madd(Dense(total_words, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\dtensor\\utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[1;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[39mif\u001b[39;00m layout:\n\u001b[0;32m     94\u001b[0m             layout_args[variable_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_layout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m layout\n\u001b[1;32m---> 96\u001b[0m init_method(layer_instance, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     98\u001b[0m \u001b[39m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m layout_param_name, layout \u001b[39min\u001b[39;00m layout_args\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:117\u001b[0m, in \u001b[0;36mDense.__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39m@utils\u001b[39m\u001b[39m.\u001b[39mallow_initializer_layout\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    116\u001b[0m ):\n\u001b[1;32m--> 117\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(activity_regularizer\u001b[39m=\u001b[39;49mactivity_regularizer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(units) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(units, \u001b[39mint\u001b[39m) \u001b[39melse\u001b[39;00m units\n\u001b[0;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    205\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer.py:340\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m allowed_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    330\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_dim\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    331\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimplementation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    338\u001b[0m }\n\u001b[0;32m    339\u001b[0m \u001b[39m# Validate optional keyword arguments.\u001b[39;00m\n\u001b[1;32m--> 340\u001b[0m generic_utils\u001b[39m.\u001b[39;49mvalidate_kwargs(kwargs, allowed_kwargs)\n\u001b[0;32m    342\u001b[0m \u001b[39m# Mutable properties\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[39m# Indicates whether the layer's weights are updated during training\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[39m# and whether the layer's updates are run during training.\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[0;32m    346\u001b[0m     \u001b[39misinstance\u001b[39m(trainable, \u001b[39mbool\u001b[39m)\n\u001b[0;32m    347\u001b[0m     \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m     )\n\u001b[0;32m    351\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\generic_utils.py:514\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[1;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39mfor\u001b[39;00m kwarg \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    513\u001b[0m     \u001b[39mif\u001b[39;00m kwarg \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_kwargs:\n\u001b[1;32m--> 514\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(error_message, kwarg)\n",
      "\u001b[1;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'input_length')"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_shape=(max_sequence_len-1,), activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 5.4822 - accuracy: 0.5600\n",
      "Epoch 2/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 2.5020 - accuracy: 0.5915\n",
      "Epoch 3/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 2.2336 - accuracy: 0.6135\n",
      "Epoch 4/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.9944 - accuracy: 0.6430\n",
      "Epoch 5/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.7335 - accuracy: 0.6889\n",
      "Epoch 6/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.5069 - accuracy: 0.7276\n",
      "Epoch 7/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.3628 - accuracy: 0.7385\n",
      "Epoch 8/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.2860 - accuracy: 0.7418\n",
      "Epoch 9/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.2456 - accuracy: 0.7429\n",
      "Epoch 10/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.2184 - accuracy: 0.7437\n",
      "Epoch 11/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.2009 - accuracy: 0.7446\n",
      "Epoch 12/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1892 - accuracy: 0.7452\n",
      "Epoch 13/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1809 - accuracy: 0.7457\n",
      "Epoch 14/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1740 - accuracy: 0.7445\n",
      "Epoch 15/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1676 - accuracy: 0.7455\n",
      "Epoch 16/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1639 - accuracy: 0.7459\n",
      "Epoch 17/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1608 - accuracy: 0.7450\n",
      "Epoch 18/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1582 - accuracy: 0.7459\n",
      "Epoch 19/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1566 - accuracy: 0.7456\n",
      "Epoch 20/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1535 - accuracy: 0.7458\n",
      "Epoch 21/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1527 - accuracy: 0.7464\n",
      "Epoch 22/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1512 - accuracy: 0.7451\n",
      "Epoch 23/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1497 - accuracy: 0.7459\n",
      "Epoch 24/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1482 - accuracy: 0.7461\n",
      "Epoch 25/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1482 - accuracy: 0.7470\n",
      "Epoch 26/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1470 - accuracy: 0.7454\n",
      "Epoch 27/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1463 - accuracy: 0.7460\n",
      "Epoch 28/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1459 - accuracy: 0.7470\n",
      "Epoch 29/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1444 - accuracy: 0.7467\n",
      "Epoch 30/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1458 - accuracy: 0.7462\n",
      "Epoch 31/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1445 - accuracy: 0.7465\n",
      "Epoch 32/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1435 - accuracy: 0.7463\n",
      "Epoch 33/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1422 - accuracy: 0.7468\n",
      "Epoch 34/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1428 - accuracy: 0.7469\n",
      "Epoch 35/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1427 - accuracy: 0.7468\n",
      "Epoch 36/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1433 - accuracy: 0.7455\n",
      "Epoch 37/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1418 - accuracy: 0.7469\n",
      "Epoch 38/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1424 - accuracy: 0.7471\n",
      "Epoch 39/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1420 - accuracy: 0.7459\n",
      "Epoch 40/100\n",
      "71/71 [==============================] - 1s 8ms/step - loss: 1.1419 - accuracy: 0.7467\n",
      "Epoch 41/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1.1417 - accuracy: 0.7468\n",
      "Epoch 42/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1407 - accuracy: 0.7468\n",
      "Epoch 43/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1411 - accuracy: 0.7467\n",
      "Epoch 44/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1413 - accuracy: 0.7470\n",
      "Epoch 45/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1414 - accuracy: 0.7478\n",
      "Epoch 46/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1411 - accuracy: 0.7467\n",
      "Epoch 47/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1409 - accuracy: 0.7465\n",
      "Epoch 48/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1405 - accuracy: 0.7457\n",
      "Epoch 49/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1.1401 - accuracy: 0.7463\n",
      "Epoch 50/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1404 - accuracy: 0.7466\n",
      "Epoch 51/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1408 - accuracy: 0.7463\n",
      "Epoch 52/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1406 - accuracy: 0.7470\n",
      "Epoch 53/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1393 - accuracy: 0.7477\n",
      "Epoch 54/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1399 - accuracy: 0.7468\n",
      "Epoch 55/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1403 - accuracy: 0.7460\n",
      "Epoch 56/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1.1395 - accuracy: 0.7463\n",
      "Epoch 57/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1389 - accuracy: 0.7473\n",
      "Epoch 58/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1389 - accuracy: 0.7475\n",
      "Epoch 59/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1384 - accuracy: 0.7470\n",
      "Epoch 60/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1.1384 - accuracy: 0.7469\n",
      "Epoch 61/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1381 - accuracy: 0.7469\n",
      "Epoch 62/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1391 - accuracy: 0.7472\n",
      "Epoch 63/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1385 - accuracy: 0.7474\n",
      "Epoch 64/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1380 - accuracy: 0.7470\n",
      "Epoch 65/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1378 - accuracy: 0.7462\n",
      "Epoch 66/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1384 - accuracy: 0.7471\n",
      "Epoch 67/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1377 - accuracy: 0.7471\n",
      "Epoch 68/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1375 - accuracy: 0.7482\n",
      "Epoch 69/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1383 - accuracy: 0.7466\n",
      "Epoch 70/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1380 - accuracy: 0.7468\n",
      "Epoch 71/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1378 - accuracy: 0.7471\n",
      "Epoch 72/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1382 - accuracy: 0.7471\n",
      "Epoch 73/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1376 - accuracy: 0.7475\n",
      "Epoch 74/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1368 - accuracy: 0.7485\n",
      "Epoch 75/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1372 - accuracy: 0.7474\n",
      "Epoch 76/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1372 - accuracy: 0.7473\n",
      "Epoch 77/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1373 - accuracy: 0.7473\n",
      "Epoch 78/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1368 - accuracy: 0.7471\n",
      "Epoch 79/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1371 - accuracy: 0.7471\n",
      "Epoch 80/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1369 - accuracy: 0.7474\n",
      "Epoch 81/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1369 - accuracy: 0.7468\n",
      "Epoch 82/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1361 - accuracy: 0.7475\n",
      "Epoch 83/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1365 - accuracy: 0.7471\n",
      "Epoch 84/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1361 - accuracy: 0.7476\n",
      "Epoch 85/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1362 - accuracy: 0.7471\n",
      "Epoch 86/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1364 - accuracy: 0.7480\n",
      "Epoch 87/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1364 - accuracy: 0.7470\n",
      "Epoch 88/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1365 - accuracy: 0.7466\n",
      "Epoch 89/100\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1.1363 - accuracy: 0.7470\n",
      "Epoch 90/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1365 - accuracy: 0.7470\n",
      "Epoch 91/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1362 - accuracy: 0.7479\n",
      "Epoch 92/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1357 - accuracy: 0.7480\n",
      "Epoch 93/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1.1359 - accuracy: 0.7461\n",
      "Epoch 94/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1.1355 - accuracy: 0.7464\n",
      "Epoch 95/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1.1352 - accuracy: 0.7479\n",
      "Epoch 96/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1.1351 - accuracy: 0.7478\n",
      "Epoch 97/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1356 - accuracy: 0.7481\n",
      "Epoch 98/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1.1357 - accuracy: 0.7481\n",
      "Epoch 99/100\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 1.1353 - accuracy: 0.7469\n",
      "Epoch 100/100\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 1.1347 - accuracy: 0.7482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23ca44aeb50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model for JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lusti\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_dir = \"./tfjs_models/FFNN\" \n",
    "tfjs.converters.save_keras_model(model, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordIndex = {}\n",
    "for word in mytokenizer.word_index:\n",
    "    wordIndex[word] = mytokenizer.word_index[word]\n",
    "\n",
    "with open('./tfjs_models/FFNN/wordIndex.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(wordIndex, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Max Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./tfjs_models/FFNN/max_sequence_len.txt', 'w') as f:\n",
    "    f.write(str(max_sequence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
