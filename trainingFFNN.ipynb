{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import json\n",
    "import tensorflowjs as tfjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./trainingData/sherlockS.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 150)               2400      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 150)               22650     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 150)               22650     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 930)               140430    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 188130 (734.88 KB)\n",
      "Trainable params: 188130 (734.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_shape=(max_sequence_len-1,), activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "71/71 [==============================] - 1s 2ms/step - loss: 0.9012 - accuracy: 0.7289\n",
      "Epoch 2/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8572 - accuracy: 0.7484\n",
      "Epoch 3/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8444 - accuracy: 0.7471\n",
      "Epoch 4/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8757 - accuracy: 0.7284\n",
      "Epoch 5/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8542 - accuracy: 0.7387\n",
      "Epoch 6/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8600 - accuracy: 0.7391\n",
      "Epoch 7/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9327 - accuracy: 0.7351\n",
      "Epoch 8/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0006 - accuracy: 0.7173\n",
      "Epoch 9/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0517 - accuracy: 0.7080\n",
      "Epoch 10/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0244 - accuracy: 0.7236\n",
      "Epoch 11/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8767 - accuracy: 0.7369\n",
      "Epoch 12/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8298 - accuracy: 0.7569\n",
      "Epoch 13/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7383 - accuracy: 0.7644\n",
      "Epoch 14/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7446 - accuracy: 0.7618\n",
      "Epoch 15/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7315 - accuracy: 0.7742\n",
      "Epoch 16/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7508 - accuracy: 0.7676\n",
      "Epoch 17/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8529 - accuracy: 0.7547\n",
      "Epoch 18/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8453 - accuracy: 0.7471\n",
      "Epoch 19/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8565 - accuracy: 0.7533\n",
      "Epoch 20/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8893 - accuracy: 0.7356\n",
      "Epoch 21/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0109 - accuracy: 0.7293\n",
      "Epoch 22/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.1267 - accuracy: 0.7027\n",
      "Epoch 23/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.2267 - accuracy: 0.6929\n",
      "Epoch 24/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9775 - accuracy: 0.7129\n",
      "Epoch 25/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8124 - accuracy: 0.7449\n",
      "Epoch 26/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7797 - accuracy: 0.7640\n",
      "Epoch 27/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7690 - accuracy: 0.7698\n",
      "Epoch 28/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7283 - accuracy: 0.7720\n",
      "Epoch 29/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7106 - accuracy: 0.7764\n",
      "Epoch 30/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7124 - accuracy: 0.7729\n",
      "Epoch 31/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7415 - accuracy: 0.7644\n",
      "Epoch 32/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7476 - accuracy: 0.7680\n",
      "Epoch 33/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7449 - accuracy: 0.7613\n",
      "Epoch 34/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7378 - accuracy: 0.7689\n",
      "Epoch 35/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7156 - accuracy: 0.7680\n",
      "Epoch 36/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7737 - accuracy: 0.7658\n",
      "Epoch 37/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7033 - accuracy: 0.7782\n",
      "Epoch 38/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7266 - accuracy: 0.7720\n",
      "Epoch 39/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7157 - accuracy: 0.7751\n",
      "Epoch 40/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8012 - accuracy: 0.7516\n",
      "Epoch 41/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8476 - accuracy: 0.7480\n",
      "Epoch 42/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9403 - accuracy: 0.7253\n",
      "Epoch 43/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0757 - accuracy: 0.7067\n",
      "Epoch 44/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.3301 - accuracy: 0.6911\n",
      "Epoch 45/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.3173 - accuracy: 0.6867\n",
      "Epoch 46/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0544 - accuracy: 0.7133\n",
      "Epoch 47/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8740 - accuracy: 0.7427\n",
      "Epoch 48/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7206 - accuracy: 0.7724\n",
      "Epoch 49/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7319 - accuracy: 0.7689\n",
      "Epoch 50/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7695 - accuracy: 0.7613\n",
      "Epoch 51/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7945 - accuracy: 0.7604\n",
      "Epoch 52/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7667 - accuracy: 0.7613\n",
      "Epoch 53/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7795 - accuracy: 0.7591\n",
      "Epoch 54/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8281 - accuracy: 0.7556\n",
      "Epoch 55/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9168 - accuracy: 0.7516\n",
      "Epoch 56/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8907 - accuracy: 0.7618\n",
      "Epoch 57/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7681 - accuracy: 0.7547\n",
      "Epoch 58/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7506 - accuracy: 0.7658\n",
      "Epoch 59/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7691 - accuracy: 0.7600\n",
      "Epoch 60/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7391 - accuracy: 0.7627\n",
      "Epoch 61/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7199 - accuracy: 0.7716\n",
      "Epoch 62/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7524 - accuracy: 0.7720\n",
      "Epoch 63/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7247 - accuracy: 0.7680\n",
      "Epoch 64/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7117 - accuracy: 0.7742\n",
      "Epoch 65/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8179 - accuracy: 0.7671\n",
      "Epoch 66/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7535 - accuracy: 0.7702\n",
      "Epoch 67/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7574 - accuracy: 0.7653\n",
      "Epoch 68/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7875 - accuracy: 0.7582\n",
      "Epoch 69/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7619 - accuracy: 0.7684\n",
      "Epoch 70/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7327 - accuracy: 0.7609\n",
      "Epoch 71/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8792 - accuracy: 0.7396\n",
      "Epoch 72/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8345 - accuracy: 0.7462\n",
      "Epoch 73/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9045 - accuracy: 0.7369\n",
      "Epoch 74/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9430 - accuracy: 0.7244\n",
      "Epoch 75/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9319 - accuracy: 0.7218\n",
      "Epoch 76/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8713 - accuracy: 0.7489\n",
      "Epoch 77/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8095 - accuracy: 0.7502\n",
      "Epoch 78/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7514 - accuracy: 0.7631\n",
      "Epoch 79/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7269 - accuracy: 0.7676\n",
      "Epoch 80/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7912 - accuracy: 0.7653\n",
      "Epoch 81/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8552 - accuracy: 0.7436\n",
      "Epoch 82/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9475 - accuracy: 0.7342\n",
      "Epoch 83/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0030 - accuracy: 0.7124\n",
      "Epoch 84/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9952 - accuracy: 0.7182\n",
      "Epoch 85/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9768 - accuracy: 0.7338\n",
      "Epoch 86/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9953 - accuracy: 0.7231\n",
      "Epoch 87/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0294 - accuracy: 0.7173\n",
      "Epoch 88/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0544 - accuracy: 0.7173\n",
      "Epoch 89/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8834 - accuracy: 0.7391\n",
      "Epoch 90/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8063 - accuracy: 0.7498\n",
      "Epoch 91/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7617 - accuracy: 0.7604\n",
      "Epoch 92/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7002 - accuracy: 0.7773\n",
      "Epoch 93/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6949 - accuracy: 0.7813\n",
      "Epoch 94/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6647 - accuracy: 0.7831\n",
      "Epoch 95/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6806 - accuracy: 0.7747\n",
      "Epoch 96/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6842 - accuracy: 0.7822\n",
      "Epoch 97/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6745 - accuracy: 0.7871\n",
      "Epoch 98/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6952 - accuracy: 0.7884\n",
      "Epoch 99/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7404 - accuracy: 0.7711\n",
      "Epoch 100/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.7800\n",
      "Epoch 101/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6631 - accuracy: 0.7836\n",
      "Epoch 102/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7153 - accuracy: 0.7716\n",
      "Epoch 103/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7251 - accuracy: 0.7671\n",
      "Epoch 104/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6823 - accuracy: 0.7831\n",
      "Epoch 105/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7111 - accuracy: 0.7756\n",
      "Epoch 106/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7738 - accuracy: 0.7684\n",
      "Epoch 107/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9898 - accuracy: 0.7249\n",
      "Epoch 108/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8561 - accuracy: 0.7444\n",
      "Epoch 109/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8815 - accuracy: 0.7378\n",
      "Epoch 110/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0840 - accuracy: 0.7191\n",
      "Epoch 111/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.1638 - accuracy: 0.7156\n",
      "Epoch 112/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.2160 - accuracy: 0.6836\n",
      "Epoch 113/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0610 - accuracy: 0.7027\n",
      "Epoch 114/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0714 - accuracy: 0.7116\n",
      "Epoch 115/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9585 - accuracy: 0.7218\n",
      "Epoch 116/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8356 - accuracy: 0.7524\n",
      "Epoch 117/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7347 - accuracy: 0.7760\n",
      "Epoch 118/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7247 - accuracy: 0.7831\n",
      "Epoch 119/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6778 - accuracy: 0.7800\n",
      "Epoch 120/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6823 - accuracy: 0.7840\n",
      "Epoch 121/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6342 - accuracy: 0.7938\n",
      "Epoch 122/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6474 - accuracy: 0.7929\n",
      "Epoch 123/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6724 - accuracy: 0.7849\n",
      "Epoch 124/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6458 - accuracy: 0.7898\n",
      "Epoch 125/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6465 - accuracy: 0.7898\n",
      "Epoch 126/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6306 - accuracy: 0.7920\n",
      "Epoch 127/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6407 - accuracy: 0.7920\n",
      "Epoch 128/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6581 - accuracy: 0.7907\n",
      "Epoch 129/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6607 - accuracy: 0.7836\n",
      "Epoch 130/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7494 - accuracy: 0.7733\n",
      "Epoch 131/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7240 - accuracy: 0.7693\n",
      "Epoch 132/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7922 - accuracy: 0.7604\n",
      "Epoch 133/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7827 - accuracy: 0.7578\n",
      "Epoch 134/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7761 - accuracy: 0.7538\n",
      "Epoch 135/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8187 - accuracy: 0.7564\n",
      "Epoch 136/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9379 - accuracy: 0.7444\n",
      "Epoch 137/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.2368 - accuracy: 0.7009\n",
      "Epoch 138/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.4481 - accuracy: 0.6791\n",
      "Epoch 139/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.1435 - accuracy: 0.6924\n",
      "Epoch 140/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0869 - accuracy: 0.7151\n",
      "Epoch 141/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7957 - accuracy: 0.7520\n",
      "Epoch 142/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6907 - accuracy: 0.7778\n",
      "Epoch 143/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.7947\n",
      "Epoch 144/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6728 - accuracy: 0.7769\n",
      "Epoch 145/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6548 - accuracy: 0.7893\n",
      "Epoch 146/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6325 - accuracy: 0.7964\n",
      "Epoch 147/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6608 - accuracy: 0.7893\n",
      "Epoch 148/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6415 - accuracy: 0.7907\n",
      "Epoch 149/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.7756\n",
      "Epoch 150/150\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6738 - accuracy: 0.7862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28dcf10ba10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=150, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model for JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir = \"./tfjs_models/FFNN1\" \n",
    "tfjs.converters.save_keras_model(model, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordIndex = {}\n",
    "for word in tokenizer.word_index:\n",
    "    wordIndex[word] = tokenizer.word_index[word]\n",
    "\n",
    "with open('./tokenizer/wordIndex.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(wordIndex, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Max Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./tokenizer/max_sequence_len.txt', 'w') as f:\n",
    "    f.write(str(max_sequence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
